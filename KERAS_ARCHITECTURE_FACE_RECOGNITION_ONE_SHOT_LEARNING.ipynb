{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from utils import LRN2D\n",
    "# import utils\n",
    "# import face_recognition\n",
    "# import dlib\n",
    "# import scipy.cluster.hierarchy as sch\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "# from matplotlib import pyplot as plt\n",
    "# from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "# import pafy\n",
    "# import imutils\n",
    "import glob, os, shutil\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTransformer():\n",
    "\n",
    "    def __init__(self, fps=30):\n",
    "        self.fps = fps\n",
    "\n",
    "    def video_to_frame(self, path_video, dir_frames):\n",
    "\n",
    "        vidcap = cv2.VideoCapture(path_video)\n",
    "        success,image = vidcap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            cv2.imwrite(dir_frames + \"/frame_\" + str(count).zfill(10) + \".jpg\", image)     # save frame as JPEG file      \n",
    "            success,image = vidcap.read()\n",
    "            #print('Read a new frame: ', success)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "    def frame_to_video(self, dir_frames, path_video, fps):\n",
    "        num_img = []\n",
    "        images = []\n",
    "        for img in os.listdir(dir_frames):\n",
    "            if img.endswith(\".png\") or img.endswith(\".jpg\"):\n",
    "                # using List comprehension + isdigit() +split() \n",
    "                # getting numbers from string  \n",
    "                data = img.split(\"_\")\n",
    "                data = data[1].split(\".\")\n",
    "                num = data[0]\n",
    "                #print(num)\n",
    "                num_img.append(num)\n",
    "                images.append(img)\n",
    "\n",
    "        sorted_numbers = numpy.argsort(num_img)\n",
    "        images_sorted = []\n",
    "        #print(images)\n",
    "        for i in range(len(images)):\n",
    "            images_sorted.append(images[sorted_numbers[i]])\n",
    "\n",
    "        frame = cv2.imread(os.path.join(dir_frames, images_sorted[0]))\n",
    "        height, width, layers = frame.shape\n",
    "        fourrc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        video = cv2.VideoWriter(path_video, fourrc, fps, (width, height))\n",
    "\n",
    "        for image in images_sorted:\n",
    "            frame = cv2.imread(os.path.join(dir_frames, image))\n",
    "            video.write(frame)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "PATH_VID = \"/home/john/dlib_files/sample1.mp4\"# location video \n",
    "DIR_FRAMES = \"/home/john/dlib_files/frames\"# directory to store frames (will be created)\n",
    "PATH_VID_AN = \"/home/john/dlib_files/output/sample_output_facenet2.mp4\"# name of annotated video\n",
    "DIR_FRAMES_AN = \"/home/john/dlib_files/labeled_frames/\"# directory to store annotated frames (will be created)\n",
    "VID_FPS = 30\n",
    "\n",
    "print(\"running\")\n",
    "\n",
    "# Create objects\n",
    "# img_annotator = ImageAnnotator(PATH_CRED)\n",
    "video_transf = VideoTransformer(VID_FPS)\n",
    "\n",
    "# create folder for frames\n",
    "try:\n",
    "    os.mkdir(DIR_FRAMES)\n",
    "except:\n",
    "    print(\"Folder \" + DIR_FRAMES + \" already exists\")\n",
    "    for filename in os.listdir(DIR_FRAMES):\n",
    "        file_path = os.path.join(DIR_FRAMES, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "try:\n",
    "    os.mkdir(DIR_FRAMES_AN)\n",
    "except:\n",
    "    print(\"Folder \" + DIR_FRAMES_AN + \" already exists\")\n",
    "    for filename in os.listdir(DIR_FRAMES_AN):\n",
    "        file_path = os.path.join(DIR_FRAMES_AN, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "# Turn video into frames\n",
    "print(\"Turn video into frames\")\n",
    "video_transf.video_to_frame(PATH_VID, DIR_FRAMES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CustomObjectScope({'tf': tf}):\n",
    "    model = load_model('/home/john/Downloads/One_Shot_Learning/model/Keras_OpenFace_facenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "\n",
    "\n",
    "_FLOATX = 'float32'\n",
    "\n",
    "def variable(value, dtype=_FLOATX, name=None):\n",
    "  v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
    "  _get_session().run(v.initializer)\n",
    "  return v\n",
    "\n",
    "def shape(x):\n",
    "  return x.get_shape()\n",
    "\n",
    "def square(x):\n",
    "  return tf.square(x)\n",
    "\n",
    "def zeros(shape, dtype=_FLOATX, name=None):\n",
    "  return variable(np.zeros(shape), dtype, name)\n",
    "\n",
    "def concatenate(tensors, axis=-1):\n",
    "  if axis < 0:\n",
    "      axis = axis % len(tensors[0].get_shape())\n",
    "  return tf.concat(axis, tensors)\n",
    "\n",
    "def LRN2D(x):\n",
    "  return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
    "\n",
    "def conv2d_bn(\n",
    "  x,\n",
    "  layer=None,\n",
    "  cv1_out=None,\n",
    "  cv1_filter=(1, 1),\n",
    "  cv1_strides=(1, 1),\n",
    "  cv2_out=None,\n",
    "  cv2_filter=(3, 3),\n",
    "  cv2_strides=(1, 1),\n",
    "  padding=None,\n",
    "):\n",
    "  num = '' if cv2_out == None else '1'\n",
    "  tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, name=layer+'_conv'+num)(x)\n",
    "  tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
    "  tensor = Activation('relu')(tensor)\n",
    "  if padding == None:\n",
    "    return tensor\n",
    "  tensor = ZeroPadding2D(padding=padding)(tensor)\n",
    "  if cv2_out == None:\n",
    "    return tensor\n",
    "  tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, name=layer+'_conv'+'2')(tensor)\n",
    "  tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
    "  tensor = Activation('relu')(tensor)\n",
    "  return tensor\n",
    "\n",
    "weights = [\n",
    "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n",
    "  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n",
    "  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n",
    "  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n",
    "  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n",
    "  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n",
    "  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n",
    "  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n",
    "  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n",
    "  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n",
    "  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n",
    "  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n",
    "  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n",
    "  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n",
    "  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n",
    "  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n",
    "  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n",
    "  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n",
    "  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n",
    "  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n",
    "  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n",
    "  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n",
    "  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n",
    "  'dense_layer'\n",
    "]\n",
    "\n",
    "conv_shape = {\n",
    "  'conv1': [64, 3, 7, 7],\n",
    "  'conv2': [64, 64, 1, 1],\n",
    "  'conv3': [192, 64, 3, 3],\n",
    "  'inception_3a_1x1_conv': [64, 192, 1, 1],\n",
    "  'inception_3a_pool_conv': [32, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n",
    "  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n",
    "  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n",
    "  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n",
    "  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_3b_pool_conv': [64, 256, 1, 1],\n",
    "  'inception_3b_1x1_conv': [64, 256, 1, 1],\n",
    "  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n",
    "  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n",
    "  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n",
    "  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n",
    "  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n",
    "  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n",
    "  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_pool_conv': [128, 640, 1, 1],\n",
    "  'inception_4a_1x1_conv': [256, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n",
    "  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n",
    "  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n",
    "  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n",
    "  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5a_pool_conv': [96, 1024, 1, 1],\n",
    "  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n",
    "  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n",
    "  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5b_pool_conv': [96, 736, 1, 1],\n",
    "  'inception_5b_1x1_conv': [256, 736, 1, 1],\n",
    "}\n",
    "\n",
    "def load_weights():\n",
    "  # Set weights path\n",
    "  dirPath = '/home/john/dlib_files/facenet/weights'\n",
    "  fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
    "  paths = {}\n",
    "  weights_dict = {}\n",
    "\n",
    "  for n in fileNames:\n",
    "    paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
    "\n",
    "  for name in weights:\n",
    "    if 'conv' in name:\n",
    "      conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "      conv_w = np.reshape(conv_w, conv_shape[name])\n",
    "      conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
    "      conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "      weights_dict[name] = [conv_w, conv_b]     \n",
    "    elif 'bn' in name:\n",
    "      bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "      bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "      bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
    "      bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
    "      weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
    "    elif 'dense' in name:\n",
    "      dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
    "      dense_w = np.reshape(dense_w, (128, 736))\n",
    "      dense_w = np.transpose(dense_w, (1, 0))\n",
    "      dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
    "      weights_dict[name] = [dense_w, dense_b]\n",
    "\n",
    "  return weights_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_embedding(image, model):\n",
    "    #image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_AREA) \n",
    "    image = cv2.resize(image, (96, 96)) \n",
    "    img = image[...,::-1]\n",
    "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(face_image, input_embeddings, model):\n",
    "\n",
    "    embedding = image_to_embedding(face_image, model)\n",
    "    \n",
    "    minimum_distance = 200\n",
    "    name = None\n",
    "    \n",
    "    # Loop over  names and encodings.\n",
    "    for (input_name, input_embedding) in input_embeddings.items():\n",
    "        \n",
    "       \n",
    "        euclidean_distance = np.linalg.norm(embedding-input_embedding)\n",
    "        \n",
    "\n",
    "#         print('Euclidean distance from %s is %s' %(input_name, euclidean_distance))\n",
    "\n",
    "        \n",
    "        if euclidean_distance < minimum_distance:\n",
    "            minimum_distance = euclidean_distance\n",
    "            name = input_name\n",
    "    \n",
    "    if minimum_distance <= 0.8:\n",
    "        return str(name)\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def create_input_image_embeddings():\n",
    "    input_embeddings = {}\n",
    "\n",
    "    for file in glob.glob(\"/home/john/dlib_files/unique_faces/*\"):\n",
    "        person_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        image_file = cv2.imread(file, 1)\n",
    "        input_embeddings[person_name] = image_to_embedding(image_file, model)\n",
    "\n",
    "    return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces_in_images1(input_embeddings):\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier('/home/john/dlib_files/haarcascade_frontalface_default.xml')\n",
    "    unknown_counter = 0\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    for image in os.listdir(\"/home/john/dlib_files/frames_R\"):\n",
    "        image_path = os.path.join(\"/home/john/dlib_files/frames_R\",image)\n",
    "        image_name = image_path.split('/')[-1].strip('.jpg')\n",
    "        frame = cv2.imread(image_path)\n",
    "#         img = frame\n",
    "        height, width, channels = frame.shape\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # Loop through all the faces detected \n",
    "        identities = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x+w\n",
    "            y2 = y+h\n",
    "\n",
    "           \n",
    "            \n",
    "            face_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]    \n",
    "            identity = recognize_face(face_image, input_embeddings, model)\n",
    "            \n",
    "            if identity is not None:\n",
    "                img = cv2.rectangle(frame,(x1, y1),(x2, y2),(0,255,0),2)\n",
    "                cv2.putText(frame, str(identity), (x1+5,y1-5), font, 1, (0,255,0), 2,cv2.LINE_AA)\n",
    "                name = \"/home/john/dlib_files/labeled_frames/\"+ str(image_name) + '.jpg'\n",
    "                cv2.imwrite(name, frame)\n",
    "            else:\n",
    "                cv2.rectangle(frame,(x1, y1),(x2, y2), (0, 0, 255), 2)\n",
    "                name = \"/home/john/dlib_files/labeled_frames/\"+ str(image_name) + '.jpg'\n",
    "                cv2.imwrite(name, frame)\n",
    "        name = \"/home/john/dlib_files/labeled_frames/\"+ str(image_name) + '.jpg'\n",
    "        cv2.imwrite(name, frame)\n",
    "\n",
    "        key = cv2.waitKey(100)\n",
    "        cv2.imshow(\"Face Recognizer\", frame)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "#     vc.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "class VideoTransformer():\n",
    "\n",
    "    def __init__(self, fps=30):\n",
    "        self.fps = fps\n",
    "\n",
    "    def video_to_frame(self, path_video, dir_frames):\n",
    "\n",
    "        vidcap = cv2.VideoCapture(path_video)\n",
    "        success,image = vidcap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            cv2.imwrite(dir_frames + \"/frame_\" + str(count).zfill(10) + \".jpg\", image)     # save frame as JPEG file      \n",
    "            success,image = vidcap.read()\n",
    "            #print('Read a new frame: ', success)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "    def frame_to_video(self, dir_frames, path_video, fps):\n",
    "        num_img = []\n",
    "        images = []\n",
    "        for img in os.listdir(dir_frames):\n",
    "            if img.endswith(\".png\") or img.endswith(\".jpg\"):\n",
    "                # using List comprehension + isdigit() +split() \n",
    "                # getting numbers from string  \n",
    "                data = img.split(\"_\")\n",
    "                data = data[1].split(\".\")\n",
    "                num = data[0]\n",
    "                #print(num)\n",
    "                num_img.append(num)\n",
    "                images.append(img)\n",
    "\n",
    "        sorted_numbers = numpy.argsort(num_img)\n",
    "        images_sorted = []\n",
    "        #print(images)\n",
    "        for i in range(len(images)):\n",
    "            images_sorted.append(images[sorted_numbers[i]])\n",
    "\n",
    "        frame = cv2.imread(os.path.join(dir_frames, images_sorted[0]))\n",
    "        height, width, layers = frame.shape\n",
    "        fourrc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        video = cv2.VideoWriter(path_video, fourrc, fps, (width, height))\n",
    "\n",
    "        for image in images_sorted:\n",
    "            frame = cv2.imread(os.path.join(dir_frames, image))\n",
    "            video.write(frame)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "PATH_VID = \"/home/john/dlib_files/sample1.mp4\"# location video \n",
    "DIR_FRAMES = \"/home/john/dlib_files/frames_R\"# directory to store frames (will be created)\n",
    "PATH_VID_AN = \"/home/john/dlib_files/output/sample_output_facenet2.mp4\"# name of annotated video\n",
    "DIR_FRAMES_AN = \"/home/john/dlib_files/labeled_frames/\"# directory to store annotated frames (will be created)\n",
    "VID_FPS = 30\n",
    "\n",
    "print(\"running\")\n",
    "\n",
    "# Create objects\n",
    "# img_annotator = ImageAnnotator(PATH_CRED)\n",
    "video_transf = VideoTransformer(VID_FPS)\n",
    "\n",
    "# create folder for frames\n",
    "try:\n",
    "    os.mkdir(DIR_FRAMES)\n",
    "except:\n",
    "    print(\"Folder \" + DIR_FRAMES + \" already exists\")\n",
    "    for filename in os.listdir(DIR_FRAMES):\n",
    "        file_path = os.path.join(DIR_FRAMES, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "try:\n",
    "    os.mkdir(DIR_FRAMES_AN)\n",
    "except:\n",
    "    print(\"Folder \" + DIR_FRAMES_AN + \" already exists\")\n",
    "    for filename in os.listdir(DIR_FRAMES_AN):\n",
    "        file_path = os.path.join(DIR_FRAMES_AN, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "# Turn video into frames\n",
    "print(\"Turn video into frames\")\n",
    "video_transf.video_to_frame(PATH_VID, DIR_FRAMES)\n",
    "\n",
    "\n",
    "\n",
    "# import packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from utils import LRN2D\n",
    "# import utils\n",
    "# import face_recognition\n",
    "# import dlib\n",
    "# import scipy.cluster.hierarchy as sch\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "# from matplotlib import pyplot as plt\n",
    "# from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "# import pafy\n",
    "# import imutils\n",
    "import glob, os, shutil\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "import tensorflow as tf\n",
    "\n",
    "with CustomObjectScope({'tf': tf}):\n",
    "    model = load_model('/home/john/Downloads/One_Shot_Learning/model/Keras_OpenFace_facenet.h5')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "\n",
    "\n",
    "_FLOATX = 'float32'\n",
    "\n",
    "def variable(value, dtype=_FLOATX, name=None):\n",
    "  v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
    "  _get_session().run(v.initializer)\n",
    "  return v\n",
    "\n",
    "def shape(x):\n",
    "  return x.get_shape()\n",
    "\n",
    "def square(x):\n",
    "  return tf.square(x)\n",
    "\n",
    "def zeros(shape, dtype=_FLOATX, name=None):\n",
    "  return variable(np.zeros(shape), dtype, name)\n",
    "\n",
    "def concatenate(tensors, axis=-1):\n",
    "  if axis < 0:\n",
    "      axis = axis % len(tensors[0].get_shape())\n",
    "  return tf.concat(axis, tensors)\n",
    "\n",
    "def LRN2D(x):\n",
    "  return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
    "\n",
    "def conv2d_bn(\n",
    "  x,\n",
    "  layer=None,\n",
    "  cv1_out=None,\n",
    "  cv1_filter=(1, 1),\n",
    "  cv1_strides=(1, 1),\n",
    "  cv2_out=None,\n",
    "  cv2_filter=(3, 3),\n",
    "  cv2_strides=(1, 1),\n",
    "  padding=None,\n",
    "):\n",
    "  num = '' if cv2_out == None else '1'\n",
    "  tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, name=layer+'_conv'+num)(x)\n",
    "  tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
    "  tensor = Activation('relu')(tensor)\n",
    "  if padding == None:\n",
    "    return tensor\n",
    "  tensor = ZeroPadding2D(padding=padding)(tensor)\n",
    "  if cv2_out == None:\n",
    "    return tensor\n",
    "  tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, name=layer+'_conv'+'2')(tensor)\n",
    "  tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
    "  tensor = Activation('relu')(tensor)\n",
    "  return tensor\n",
    "\n",
    "weights = [\n",
    "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n",
    "  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n",
    "  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n",
    "  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n",
    "  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n",
    "  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n",
    "  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n",
    "  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n",
    "  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n",
    "  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n",
    "  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n",
    "  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n",
    "  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n",
    "  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n",
    "  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n",
    "  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n",
    "  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n",
    "  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n",
    "  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n",
    "  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n",
    "  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n",
    "  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n",
    "  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n",
    "  'dense_layer'\n",
    "]\n",
    "\n",
    "conv_shape = {\n",
    "  'conv1': [64, 3, 7, 7],\n",
    "  'conv2': [64, 64, 1, 1],\n",
    "  'conv3': [192, 64, 3, 3],\n",
    "  'inception_3a_1x1_conv': [64, 192, 1, 1],\n",
    "  'inception_3a_pool_conv': [32, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n",
    "  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n",
    "  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n",
    "  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n",
    "  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_3b_pool_conv': [64, 256, 1, 1],\n",
    "  'inception_3b_1x1_conv': [64, 256, 1, 1],\n",
    "  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n",
    "  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n",
    "  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n",
    "  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n",
    "  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n",
    "  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n",
    "  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_pool_conv': [128, 640, 1, 1],\n",
    "  'inception_4a_1x1_conv': [256, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n",
    "  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n",
    "  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n",
    "  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n",
    "  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5a_pool_conv': [96, 1024, 1, 1],\n",
    "  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n",
    "  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n",
    "  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5b_pool_conv': [96, 736, 1, 1],\n",
    "  'inception_5b_1x1_conv': [256, 736, 1, 1],\n",
    "}\n",
    "\n",
    "def load_weights():\n",
    "  # Set weights path\n",
    "  dirPath = '/home/john/dlib_files/facenet/weights'\n",
    "  fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
    "  paths = {}\n",
    "  weights_dict = {}\n",
    "\n",
    "  for n in fileNames:\n",
    "    paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
    "\n",
    "  for name in weights:\n",
    "    if 'conv' in name:\n",
    "      conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "      conv_w = np.reshape(conv_w, conv_shape[name])\n",
    "      conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
    "      conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "      weights_dict[name] = [conv_w, conv_b]     \n",
    "    elif 'bn' in name:\n",
    "      bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "      bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "      bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
    "      bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
    "      weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
    "    elif 'dense' in name:\n",
    "      dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
    "      dense_w = np.reshape(dense_w, (128, 736))\n",
    "      dense_w = np.transpose(dense_w, (1, 0))\n",
    "      dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
    "      weights_dict[name] = [dense_w, dense_b]\n",
    "\n",
    "  return weights_dict\n",
    "\n",
    "\n",
    "\n",
    "def image_to_embedding(image, model):\n",
    "    #image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_AREA) \n",
    "    image = cv2.resize(image, (96, 96)) \n",
    "    img = image[...,::-1]\n",
    "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding\n",
    "\n",
    "def recognize_face(face_image, input_embeddings, model):\n",
    "\n",
    "    embedding = image_to_embedding(face_image, model)\n",
    "    \n",
    "    minimum_distance = 200\n",
    "    name = None\n",
    "    \n",
    "    # Loop over  names and encodings.\n",
    "    for (input_name, input_embedding) in input_embeddings.items():\n",
    "        \n",
    "       \n",
    "        euclidean_distance = np.linalg.norm(embedding-input_embedding)\n",
    "        \n",
    "\n",
    "#         print('Euclidean distance from %s is %s' %(input_name, euclidean_distance))\n",
    "\n",
    "        \n",
    "        if euclidean_distance < minimum_distance:\n",
    "            minimum_distance = euclidean_distance\n",
    "            name = input_name\n",
    "    \n",
    "    if minimum_distance <= 0.8:\n",
    "        return str(name)\n",
    "    else:\n",
    "        return \n",
    "\n",
    "import glob\n",
    "\n",
    "def create_input_image_embeddings():\n",
    "    input_embeddings = {}\n",
    "\n",
    "    for file in glob.glob(\"/home/john/dlib_files/unique_faces/*\"):\n",
    "        person_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        image_file = cv2.imread(file, 1)\n",
    "        input_embeddings[person_name] = image_to_embedding(image_file, model)\n",
    "\n",
    "    return input_embeddings\n",
    "\n",
    "def recognize_faces_in_images1(input_embeddings):\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier('/home/john/dlib_files/haarcascade_frontalface_default.xml')\n",
    "    unknown_counter = 0\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    for image in os.listdir(\"/home/john/dlib_files/frames\"):\n",
    "        image_path = os.path.join(\"/home/john/dlib_files/frames\",image)\n",
    "        image_name = image_path.split('/')[-1].strip('.jpg')\n",
    "        frame = cv2.imread(image_path)\n",
    "#         img = frame\n",
    "        height, width, channels = frame.shape\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # Loop through all the faces detected \n",
    "        identities = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x+w\n",
    "            y2 = y+h\n",
    "\n",
    "           \n",
    "            \n",
    "            face_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]    \n",
    "            identity = recognize_face(face_image, input_embeddings, model)\n",
    "            \n",
    "            if identity is not None:\n",
    "                img = cv2.rectangle(frame,(x1, y1),(x2, y2),(0,255,0),2)\n",
    "                cv2.putText(frame, str(identity), (x1+5,y1-5), font, 1, (0,255,0), 2,cv2.LINE_AA)\n",
    "                name = \"/home/john/dlib_files/labeled_frames/\"+ str(image_name) + '.jpg'\n",
    "                cv2.imwrite(name, frame)\n",
    "            else:\n",
    "                cv2.rectangle(frame,(x1, y1),(x2, y2), (0, 0, 255), 2)\n",
    "                name = \"/home/john/dlib_files/labeled_frames/\"+ str(image_name) + '.jpg'\n",
    "                cv2.imwrite(name, frame)\n",
    "        name = \"/home/john/dlib_files/labeled_frames/\"+ str(image_name) + '.jpg'\n",
    "        cv2.imwrite(name, frame)\n",
    "\n",
    "        key = cv2.waitKey(100)\n",
    "        cv2.imshow(\"Face Recognizer\", frame)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "#     vc.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "\n",
    "input_embeddings = create_input_image_embeddings()\n",
    "recognize_faces_in_images1(input_embeddings)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print(f'Finished in {t2-t1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform frames to video\n",
    "print(\"Transform frames to video\")\n",
    "video_transf.frame_to_video(DIR_FRAMES_AN, PATH_VID_AN,VID_FPS)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
